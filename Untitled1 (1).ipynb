{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d663dc9",
   "metadata": {
    "height": 931
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: selfrag.pdf\n",
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "PG19 test split\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation results\"}\n",
      "=== Function Output ===\n",
      "The evaluation results show that the models achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity values. Additionally, the models are fine-tuned on different context lengths, such as 100k, 65536, and 32768, and achieve promising results on these extremely large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA is the PG19 test split. \n",
      "\n",
      "Regarding the evaluation results, the models in LongLoRA achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity values. The models are fine-tuned on different context lengths, such as 100k, 65536, and 32768, and achieve promising results on these extremely large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n",
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of Language Models (LMs) by training them to adaptively retrieve passages on-demand, generate text informed by these retrieved passages, and reflect on their own output using special tokens called reflection tokens. This framework allows the LM to tailor its behavior to diverse task requirements during the inference phase, leading to significant performance improvements compared to state-of-the-art LMs and retrieval-augmented models on various tasks such as open-domain QA, reasoning, and fact verification.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient method for extending the context sizes of pre-trained large language models (LLMs) with limited computation cost. It combines improved LoRA with shifted sparse attention (S2-Attn) to enable effective and efficient context extension. LongLoRA demonstrates strong empirical results on various tasks, retains the original architectures of models, and is compatible with most existing techniques. It facilitates fine-tuning pre-trained LLMs to longer context lengths while maintaining performance and reducing computational costs.\n",
      "=== LLM Response ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of Language Models (LMs) by training them to adaptively retrieve passages on-demand, generate text informed by these retrieved passages, and reflect on their own output using special tokens called reflection tokens. This framework allows the LM to tailor its behavior to diverse task requirements during the inference phase, leading to significant performance improvements compared to state-of-the-art LMs and retrieval-augmented models on various tasks such as open-domain QA, reasoning, and fact verification.\n",
      "\n",
      "LongLoRA is an efficient method for extending the context sizes of pre-trained large language models (LLMs) with limited computation cost. It combines improved LoRA with shifted sparse attention (S2-Attn) to enable effective and efficient context extension. LongLoRA demonstrates strong empirical results on various tasks, retains the original architectures of models, and is compatible with most existing techniques. It facilitates fine-tuning pre-trained LLMs to longer context lengths while maintaining performance and reducing computational costs.\n",
      "assistant: Self-RAG is a framework that enhances the quality and factuality of Language Models (LMs) by training them to adaptively retrieve passages on-demand, generate text informed by these retrieved passages, and reflect on their own output using special tokens called reflection tokens. This framework allows the LM to tailor its behavior to diverse task requirements during the inference phase, leading to significant performance improvements compared to state-of-the-art LMs and retrieval-augmented models on various tasks such as open-domain QA, reasoning, and fact verification.\n",
      "\n",
      "LongLoRA is an efficient method for extending the context sizes of pre-trained large language models (LLMs) with limited computation cost. It combines improved LoRA with shifted sparse attention (S2-Attn) to enable effective and efficient context extension. LongLoRA demonstrates strong empirical results on various tasks, retains the original architectures of models, and is compatible with most existing techniques. It facilitates fine-tuning pre-trained LLMs to longer context lengths while maintaining performance and reducing computational costs.\n",
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and SoftwareDev.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in SWE-Bench includes diverse instruction-following input-output pairs sampled from Open-Instruct processed data, as well as knowledge-intensive datasets such as those from Petroni et al. (2021), Stelmakh et al. (2022), and Mihaylov et al. (2018). In total, 150k instruction-output pairs are utilized for training. Additionally, the evaluation dataset used in SWE-Bench is PopQA, PubHealth, and ASQA.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and SoftwareDev. \n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench includes diverse instruction-following input-output pairs sampled from Open-Instruct processed data, as well as knowledge-intensive datasets such as those from Petroni et al. (2021), Stelmakh et al. (2022), and Mihaylov et al. (2018). In total, 150k instruction-output pairs are utilized for training. Additionally, the evaluation dataset used in SWE-Bench includes PopQA, PubHealth, and ASQA.\n",
      "assistant: The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and SoftwareDev. \n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench includes diverse instruction-following input-output pairs sampled from Open-Instruct processed data, as well as knowledge-intensive datasets such as those from Petroni et al. (2021), Stelmakh et al. (2022), and Mihaylov et al. (2018). In total, 150k instruction-output pairs are utilized for training. Additionally, the evaluation dataset used in SWE-Bench includes PopQA, PubHealth, and ASQA.\n"
     ]
    }
   ],
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "]\n",
    "\n",
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n",
    "\n",
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)\n",
    "\n",
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")\n",
    "\n",
    "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(str(response))\n",
    "\n",
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6ca0ee",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
